{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rest_voltage: -70.0\n",
      "Added 1 external_states. See `.externals` for details.\n",
      "Added 1500 recordings. See `.recordings` for details.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training Loop\n",
    "\"\"\"\n",
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import jaxley as jx\n",
    "from jaxley.solver_gate import solve_gate_exponential, save_exp\n",
    "from jaxley.channels import Channel\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "# import jaxley.optimize.transforms as jt\n",
    "# from jax import random\n",
    "# import tensorflow_probability.substrates.jax as tfp\n",
    "# import optax\n",
    "# from jax import jit, vmap, value_and_grad\n",
    "# import jax.lax as lax\n",
    "\n",
    "SEED = 0\n",
    "GPU = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU)\n",
    "\n",
    "NUM_EPOCHS = 400\n",
    "SIM_TIME_SAMPLES = 7\n",
    "TIME_STEP = 2e-3 #ms\n",
    "TOTAL_SIM_TIME = SIM_TIME_SAMPLES * TIME_STEP #ms\n",
    "\n",
    "DUMP_PERIOD = 20\n",
    "TOTAL_LENGTH = 1500.0\n",
    "SEGMENT_LENGTH = 1.0\n",
    "assert TOTAL_LENGTH % SEGMENT_LENGTH == 0, \"TOTAL_LENGTH must be divisible by SEGMENT_LENGTH\"\n",
    "NUM_COMPARTMENTS = int(TOTAL_LENGTH/SEGMENT_LENGTH)\n",
    "VOLTAGE_CLAMP_SEGMENT = 50\n",
    "VOLTAGE_CLAMP_VALUE = 0.0\n",
    "STOP_CRITERION = 0.05\n",
    "PERCENTILE_SAMPLE_AND_CLIP = 0.85\n",
    "SAVE_NAME = f'seed_{SEED}'\n",
    "np.random.seed(SEED)\n",
    "ELEC_SPACING = 30.0\n",
    "ELECTRODE_CONFIGURATION = 'triangle'\n",
    "ELEC_COORDS = None\n",
    "\n",
    "if ELECTRODE_CONFIGURATION == 'triangle':\n",
    "    #place electrodes at equilateral traiangle in x=0 plane, center at (0,0,0), spaced ELEC_SPACING apart\n",
    "    ELEC_COORDS = np.array([[0, 0, 0], [0, 1, 0], [0, 0.5, 0.866]])\n",
    "    ELEC_COORDS = ELEC_COORDS - np.mean(ELEC_COORDS, axis=0)\n",
    "    ELEC_COORDS = ELEC_SPACING*ELEC_COORDS\n",
    "    ELEC_COORDS = jnp.array(ELEC_COORDS, dtype=jnp.float32)\n",
    "    ELEC_COORDS = jax.device_put(ELEC_COORDS) # Move to default accelerator (GPU if available)\n",
    "\n",
    "def _vtrap(x, y):\n",
    "    return x / (save_exp(x / y) - 1.0)\n",
    "\n",
    "class HH(Channel):\n",
    "    \"\"\"Hodgkin-Huxley channel.\"\"\"\n",
    "\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        self.current_is_in_mA_per_cm2 = True\n",
    "\n",
    "        super().__init__(name)\n",
    "        prefix = self._name\n",
    "        self.channel_params = {\n",
    "            f\"{prefix}_gNa\": 0.12,\n",
    "            f\"{prefix}_gK\": 0.036,\n",
    "            f\"{prefix}_gLeak\": 0.0003,\n",
    "            f\"{prefix}_eNa\": 60.0,\n",
    "            f\"{prefix}_eK\": -77.0,\n",
    "            f\"{prefix}_eLeak\": -54.3,\n",
    "        }\n",
    "        self.channel_states = {\n",
    "            f\"{prefix}_m\": 0.2,\n",
    "            f\"{prefix}_h\": 0.2,\n",
    "            f\"{prefix}_n\": 0.2,\n",
    "        }\n",
    "        self.current_name = f\"i_HH\"\n",
    "\n",
    "    def update_states(\n",
    "        self,\n",
    "        states: Dict[str, jnp.ndarray],\n",
    "        dt,\n",
    "        v,\n",
    "        params: Dict[str, jnp.ndarray],\n",
    "    ):\n",
    "        \"\"\"Return updated HH channel state.\"\"\"\n",
    "        prefix = self._name\n",
    "        m, h, n = states[f\"{prefix}_m\"], states[f\"{prefix}_h\"], states[f\"{prefix}_n\"]\n",
    "        new_m = solve_gate_exponential(m, dt, *self.m_gate(v))\n",
    "        new_h = solve_gate_exponential(h, dt, *self.h_gate(v))\n",
    "        new_n = solve_gate_exponential(n, dt, *self.n_gate(v))\n",
    "        return {f\"{prefix}_m\": new_m, f\"{prefix}_h\": new_h, f\"{prefix}_n\": new_n}\n",
    "\n",
    "    def compute_current(\n",
    "        self, states: Dict[str, jnp.ndarray], v, params: Dict[str, jnp.ndarray]\n",
    "    ):\n",
    "        \"\"\"Return current through HH channels.\"\"\"\n",
    "        prefix = self._name\n",
    "        m, h, n = states[f\"{prefix}_m\"], states[f\"{prefix}_h\"], states[f\"{prefix}_n\"]\n",
    "\n",
    "        gNa = params[f\"{prefix}_gNa\"] * (m**3) * h  # S/cm^2\n",
    "        gK = params[f\"{prefix}_gK\"] * n**4  # S/cm^2\n",
    "        gLeak = params[f\"{prefix}_gLeak\"]  # S/cm^2\n",
    "\n",
    "        return (\n",
    "            gNa * (v - params[f\"{prefix}_eNa\"])\n",
    "            + gK * (v - params[f\"{prefix}_eK\"])\n",
    "            + gLeak * (v - params[f\"{prefix}_eLeak\"])\n",
    "        )\n",
    "\n",
    "    def init_state(self, states, v, params, delta_t):\n",
    "        \"\"\"Initialize the state such at fixed point of gate dynamics.\"\"\"\n",
    "        prefix = self._name\n",
    "        alpha_m, beta_m = self.m_gate(v)\n",
    "        alpha_h, beta_h = self.h_gate(v)\n",
    "        alpha_n, beta_n = self.n_gate(v)\n",
    "        return {\n",
    "            f\"{prefix}_m\": alpha_m / (alpha_m + beta_m),\n",
    "            f\"{prefix}_h\": alpha_h / (alpha_h + beta_h),\n",
    "            f\"{prefix}_n\": alpha_n / (alpha_n + beta_n),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def m_gate(v):\n",
    "        alpha = 2.725 * _vtrap(-(v + 35), 10)\n",
    "        beta = 90.83 * save_exp(-(v + 60) / 20)\n",
    "        return alpha, beta\n",
    "\n",
    "    @staticmethod\n",
    "    def h_gate(v):\n",
    "        alpha = 1.817 * save_exp(-(v + 52) / 20)\n",
    "        beta = 27.25 / (save_exp(-(v + 22) / 10) + 1)\n",
    "        return alpha, beta\n",
    "\n",
    "    @staticmethod\n",
    "    def n_gate(v):\n",
    "        alpha = 0.09575 * _vtrap(-(v + 37), 10)\n",
    "        beta = 1.915 * save_exp(-(v + 47) / 80)\n",
    "        return alpha, beta\n",
    "\n",
    "def transform_point(x: jnp.ndarray, y: jnp.ndarray, z: jnp.ndarray, \n",
    "                   phi: jnp.ndarray, theta: jnp.ndarray) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"\n",
    "    Transform a point using phi and theta rotations.\n",
    "    \n",
    "    \"\"\"\n",
    "    final_z = z * jnp.cos(phi) - x * jnp.sin(phi)\n",
    "    int_x = x * jnp.cos(phi) + z * jnp.sin(phi)\n",
    "    final_x = int_x * jnp.cos(theta) - y * jnp.sin(theta)\n",
    "    final_y = y * jnp.cos(theta) + int_x * jnp.sin(theta)\n",
    "    return final_x, final_y, final_z\n",
    "\n",
    "def compute_cell_locations_from_orientations(\n",
    "    axon_origin_dist: jnp.ndarray,\n",
    "    phi: jnp.ndarray, \n",
    "    theta: jnp.ndarray,\n",
    "    axon_spin_angle: jnp.ndarray\n",
    ") -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    Compute cell compartment locations from orientation parameters.\n",
    "    \"\"\"\n",
    "    # Calculate original endpoints before phi and theta rotations using the spin angle\n",
    "    original_first_x = -1 * TOTAL_LENGTH / 2 * jnp.cos(axon_spin_angle)\n",
    "    original_first_y = -1 * TOTAL_LENGTH / 2 * jnp.sin(axon_spin_angle)\n",
    "    original_first_z = axon_origin_dist\n",
    "    \n",
    "    # Transform the endpoints\n",
    "    ff_x, ff_y, ff_z = transform_point(original_first_x, original_first_y, original_first_z, phi, theta)\n",
    "    fl_x, fl_y, fl_z = transform_point(-1 * original_first_x, -1 * original_first_y, original_first_z, phi, theta)\n",
    "\n",
    "    #return stacked interpolation between x,y,z endpoints in one tensor\n",
    "    return jnp.stack([\n",
    "        jnp.linspace(ff_x, fl_x, NUM_COMPARTMENTS, endpoint=False),\n",
    "        jnp.linspace(ff_y, fl_y, NUM_COMPARTMENTS, endpoint=False),\n",
    "        jnp.linspace(ff_z, fl_z, NUM_COMPARTMENTS, endpoint=False)\n",
    "    ], axis=1)\n",
    "\n",
    "\n",
    "class SimpleCellEIAndLoss:\n",
    "    def __init__(self):\n",
    "        comp = jx.Compartment()\n",
    "        branch = jx.Branch([comp]*NUM_COMPARTMENTS)\n",
    "        self.cell = jx.Cell([branch], parents=[-1])\n",
    "        self.cell.insert(HH())\n",
    "        self.cell.set(\"HH_gLeak\", 1e-4)    # Leak conductance in S/cm^2\n",
    "        self.cell.set(\"HH_eNa\", 60.60)     # Sodium reversal potential in mV\n",
    "        self.cell.set(\"HH_eK\", -101.34)     # Potassium reversal potential in mV\n",
    "        self.cell.set(\"HH_eLeak\", -64.58)  # Leak reversal potential in mV\n",
    "        self.cell.set(\"HH_m\", 0.0353)        # Initial value of m gate\n",
    "        self.cell.set(\"HH_h\", 0.9054)        # Initial value of h gate  \n",
    "        self.cell.set(\"HH_n\", 0.0677)        # Initial value of n gate\n",
    "        self.cell.set(\"length\", SEGMENT_LENGTH)  \n",
    "        # Parameter bounds (same as PyTorch version)\n",
    "        self.MODEL_BOUNDS = {\n",
    "            'axon_origin_dist': (10.0, 30.0),\n",
    "            'axon_theta': (-jnp.pi/2, jnp.pi/2),\n",
    "            'axon_phi': (0.0, jnp.pi),\n",
    "            'axon_spin_angle': (-jnp.pi/2, jnp.pi/2),\n",
    "            'fiber_radius_um': (1.0, 5.0),\n",
    "            'sodium_channel_density': (0.1, 0.3),\n",
    "            'potassium_channel_density': (0.1, 0.3),\n",
    "            'axial_resistivity': (50.0, 200.0)\n",
    "        }\n",
    "        # Parameters to optimize (same as PyTorch version)\n",
    "        self.params_list = [\n",
    "            'axon_origin_dist', 'axon_theta', \n",
    "            'axon_phi', 'axon_spin_angle',\n",
    "            'fiber_radius_um', 'sodium_channel_density',\n",
    "            'potassium_channel_density', 'axial_resistivity'\n",
    "        ]\n",
    "        # Initialize raw parameters (unconstrained, for optimization)\n",
    "        self.raw_params = self._initialize_raw_params_midpoint()\n",
    "        self.locations = jnp.zeros((NUM_COMPARTMENTS, 3))  # Initialize empty locations tensor\n",
    "\n",
    "    def _convert_to_raw(self, param_val, min_val, max_val):\n",
    "        \"\"\"Convert constrained parameter to raw (unconstrained) space.\"\"\"\n",
    "        ratio = (param_val - min_val) / (max_val - min_val)\n",
    "        # Clamp to avoid logit of 0 or 1\n",
    "        ratio = jnp.clip(ratio, 0.001, 0.999)\n",
    "        return jnp.log(ratio / (1.0 - ratio))  # logit, inverse of sigmoid\n",
    "\n",
    "    def _sigmoid_reparametrize(self, raw_val, min_val, max_val):\n",
    "        \"\"\"Convert raw parameter back to constrained space using sigmoid.\"\"\"\n",
    "        ratio = 1.0 / (1.0 + jnp.exp(-raw_val))  # sigmoid\n",
    "        return min_val + ratio * (max_val - min_val)\n",
    "\n",
    "    def _initialize_raw_params_midpoint(self):\n",
    "        \"\"\"Initialize raw parameters with midpoint values.\"\"\"\n",
    "        raw_params = {}\n",
    "        for param_name in self.params_list:\n",
    "            bounds = self.MODEL_BOUNDS[param_name]\n",
    "            # Start at midpoint of bounds\n",
    "            midpoint = (bounds[0] + bounds[1]) / 2.0\n",
    "            # Convert to raw (unconstrained) space\n",
    "            raw_val = self._convert_to_raw(midpoint, bounds[0], bounds[1])\n",
    "            raw_params[param_name] = jnp.array(raw_val)\n",
    "        return raw_params\n",
    "    \n",
    "    def set_params(self, params: Dict[str, jnp.ndarray]):\n",
    "        \"\"\"Set parameters from a dictionary.\"\"\"\n",
    "        for param_name, value in params.items():\n",
    "            self.raw_params[param_name] = self._convert_to_raw(\n",
    "                value, self.MODEL_BOUNDS[param_name][0], self.MODEL_BOUNDS[param_name][1])\n",
    "\n",
    "\n",
    "\n",
    "    def get_parameters(self):\n",
    "        \"\"\"Convert raw parameters to constrained parameters.\"\"\"\n",
    "        transformed_params = {}\n",
    "        for param_name in self.params_list:\n",
    "            bounds = self.MODEL_BOUNDS[param_name]\n",
    "            raw_val = self.raw_params[param_name]\n",
    "            transformed_val = self._sigmoid_reparametrize(raw_val, bounds[0], bounds[1])\n",
    "            transformed_params[param_name] = transformed_val\n",
    "        return transformed_params\n",
    "\n",
    "\n",
    "    def transform_params(self):\n",
    "        \"\"\"Update cell parameters based on current transformed parameters.\"\"\"\n",
    "\n",
    "        transformed_params = self.get_parameters()\n",
    "\n",
    "        self.cell.set(\"HH_gNa\", transformed_params['sodium_channel_density'])\n",
    "        self.cell.set(\"HH_gK\", transformed_params['potassium_channel_density'])\n",
    "        self.cell.set(\"axial_resistivity\", transformed_params['axial_resistivity'])\n",
    "        self.cell.set(\"radius\", transformed_params['fiber_radius_um'])\n",
    "\n",
    "        # Update cell morphology using our JAX function\n",
    "        self.locations = compute_cell_locations_from_orientations(\n",
    "            axon_origin_dist=transformed_params['axon_origin_dist'],\n",
    "            phi=transformed_params['axon_phi'],\n",
    "            theta=transformed_params['axon_theta'],\n",
    "            axon_spin_angle=transformed_params['axon_spin_angle']\n",
    "        )\n",
    "\n",
    "        self.cell.set(\"x\", self.locations[:, 0])\n",
    "        self.cell.set(\"y\", self.locations[:, 1])\n",
    "        self.cell.set(\"z\", self.locations[:, 2])\n",
    "\n",
    "    def compute_membrane_currents(self):\n",
    "        \"\"\"Compute membrane currents using current parameters.\"\"\"\n",
    "        # This would be implemented based on your specific needs\n",
    "        self.transform_params()\n",
    "        rest_voltage = self.cell.nodes[\"v\"][0]\n",
    "        print(f\"rest_voltage: {rest_voltage}\")\n",
    "        #set the starting voltage for the first CLAMP compartments to the clamp voltage\n",
    "        voltage_vector = jnp.array([VOLTAGE_CLAMP_VALUE] * VOLTAGE_CLAMP_SEGMENT + \n",
    "                                   [rest_voltage] * (NUM_COMPARTMENTS - VOLTAGE_CLAMP_SEGMENT))\n",
    "        self.cell.set(\"v\", voltage_vector)\n",
    "\n",
    "        current = jx.step_current(i_delay=1.0, i_dur=1.0, i_amp=0.0, delta_t=TIME_STEP, t_max=TOTAL_SIM_TIME)\n",
    "        self.cell.branch(0).loc(0.0).stimulate(current)\n",
    "\n",
    "        # # Record total transmembrane current\n",
    "        self.cell.record(\"v\")\n",
    "        #self.cell.record(\"i_mem\")\n",
    "        #self.cell.record(\"HH_m\")  # sodium activation\n",
    "        #self.cell.record(\"HH_h\")  # sodium inactivation \n",
    "        #self.cell.record(\"HH_n\")  # potassium activation\n",
    "        \n",
    "        # # Run simulation\n",
    "        #return jx.integrate(self.cell, delta_t=TIME_STEP)\n",
    "\n",
    "\n",
    "\n",
    "    def compute_eap(self):\n",
    "        \"\"\"Compute extracellular action potentials.\"\"\"\n",
    "        # This would be implemented based on your specific needs\n",
    "        pass\n",
    "\n",
    "cell_trainer = SimpleCellEIAndLoss()\n",
    "cell_trainer.transform_params()\n",
    "\n",
    "# Convert cell nodes to dataframe\n",
    "import pandas as pd\n",
    "# Convert cell nodes to dataframe and save as CSV\n",
    "nodes_df = pd.DataFrame(cell_trainer.cell.nodes)\n",
    "nodes_df.to_csv('/pool0/lotlikar/software/extracellular-jaxley/src/cell_nodes.csv', index=False)\n",
    "\n",
    "outputs = cell_trainer.compute_membrane_currents()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 79)\n",
      "Reshaped outputs shape: (4, 100, 79)\n",
      "[0.9054     0.86208063 0.82200843 0.785811   0.7532714  0.7235171\n",
      " 0.6955084  0.6683768  0.6415399  0.614702   0.5878066  0.5609681\n",
      " 0.5343943  0.5083169  0.48294246 0.45842806 0.43487674 0.4123445\n",
      " 0.39085147 0.37039277 0.3509471  0.3324832  0.31496397 0.2983495\n",
      " 0.28259876 0.26767072 0.25352517 0.24012294 0.22742635 0.21539925\n",
      " 0.20400704 0.19321679 0.18299711 0.1733182  0.16415174 0.15547085\n",
      " 0.14725003 0.13946514 0.13209325 0.12511268 0.11850291 0.11224447\n",
      " 0.10631897 0.10070898 0.09539807 0.09037066 0.08561207 0.08110843\n",
      " 0.07684668 0.07281452 0.06900038 0.06539343 0.0619835  0.05876115\n",
      " 0.05571759 0.05284471 0.05013508 0.04758194 0.04517924 0.04292165\n",
      " 0.04080456 0.03882414 0.03697735 0.03526196 0.03367659 0.03222069\n",
      " 0.03089455 0.02969921 0.02863639 0.02770837 0.02691779 0.02626745\n",
      " 0.02576013 0.02539835 0.02518423 0.02511953 0.02520556 0.02544333\n",
      " 0.02583371]\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape)\n",
    "# Reshape outputs to (4, 100, ...) for the 4 recorded variables over 100 timesteps\n",
    "reshaped_outputs = outputs.reshape(4, 100, -1)\n",
    "print(\"Reshaped outputs shape:\", reshaped_outputs.shape)\n",
    "\n",
    "print(reshaped_outputs[2, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaxleyTrainer:\n",
    "    def __init__(self, cell, data, epochs, dt = 0.05, tmax = 5 ):\n",
    "        # load through data\n",
    "        self.cell = cell\n",
    "        self.data_point = data\n",
    "        \n",
    "        self.i_delay = 1.0  # ms\n",
    "        self.i_dur = 0.15  # ms\n",
    "        self.i_amp = 3.9  # nA\n",
    "        self.electrode = generate_electrode_map(519)\n",
    "        self.ncomp = self.cell.shape[1]\n",
    "        self.params = self.setup_params()\n",
    "        for i, param_dict in enumerate(self.params):\n",
    "            for param_name, param_values in param_dict.items():\n",
    "                if param_name in ['radius', 'length']:\n",
    "                    print(f\"Branch {i}, {param_name}: shape={param_values.shape}\")\n",
    "                    print(f\"  min={jnp.min(param_values):.10f}\")\n",
    "                    print(f\"  max={jnp.max(param_values):.10f}\")\n",
    "                    print(f\"  mean={jnp.mean(param_values):.10f}\")\n",
    "                    print(f\"  any_nan={jnp.any(jnp.isnan(param_values))}\")\n",
    "                    print(f\"  any_inf={jnp.any(jnp.isinf(param_values))}\")\n",
    "        transforms = [{k: self.create_transforms(k) for k in param} for param in self.params]\n",
    "\n",
    "        self.tf = jt.ParamTransform(transforms)\n",
    "        self.lengths = self.cell.nodes[\"length\"] ### fix\n",
    "        self.lengths.to_numpy()\n",
    "        self.lengths = jnp.array(self.lengths)\n",
    "        self.N_ELECTRODES = LITKE_519_ARRAY_GRID.shape[0]\n",
    "        self.grid = jnp.array(LITKE_519_ARRAY_GRID)\n",
    "        self.delta_t = 0.05 # ms\n",
    "        self.t_max = 5\n",
    "        \n",
    "        self.num_epochs = epochs\n",
    "        self.current_compartment_positions = fh.compute_comp_xyz(self.cell)\n",
    "        self.compartment_surface_areas = fh.get_surface_areas(self.cell) * 10E-8  \n",
    "        self.opt_params = self.tf.inverse(self.params)\n",
    "\n",
    "        hold_steps = 15000  # Number of steps to hold the learning rate constant\n",
    "        decay_steps = 30000  # Total training steps (or however many steps you want the cosine decay to complete)\n",
    "\n",
    "        # 1. Constant learning rate for first 5000 steps\n",
    "        constant = optax.constant_schedule(value=1e-1)\n",
    "\n",
    "        # 2. Cosine decay from 1e-1 to 1e-5\n",
    "        cosine = optax.cosine_decay_schedule(\n",
    "            init_value=1e-1,\n",
    "            decay_steps=decay_steps - hold_steps,\n",
    "            alpha=1e-5 / 1e-1  # final_lr / init_lr\n",
    "        )\n",
    "\n",
    "        # Combine the schedules\n",
    "        schedule = optax.join_schedules(\n",
    "            schedules=[constant, cosine],\n",
    "            boundaries=[hold_steps]\n",
    "        )\n",
    "        self.optimizer = optax.chain(\n",
    "            optax.clip_by_global_norm(1.0), \n",
    "            optax.adam(learning_rate=schedule)\n",
    "        )\n",
    "        self.opt_state = self.optimizer.init(self.opt_params)\n",
    "        \n",
    "        self.jitted_grad = jit(value_and_grad(self.loss, argnums=0))\n",
    "\n",
    "    def compute_comp_xyz(self, transformed_params):\n",
    "        '''\n",
    "        Returns (Ncomps, 3) array of xyz positions of all the compartments\n",
    "        '''\n",
    "        \n",
    "        # \n",
    "        all_x = []\n",
    "        all_y = []\n",
    "        all_z = []\n",
    "    \n",
    "        for param_dict in transformed_params:\n",
    "            if 'x' in param_dict:\n",
    "                all_x.append(param_dict['x'])\n",
    "            if 'y' in param_dict:\n",
    "                all_y.append(param_dict['y'])\n",
    "            if 'z' in param_dict:\n",
    "                all_z.append(param_dict['z'])\n",
    "        xs = jnp.concatenate(all_x) \n",
    "        ys = jnp.concatenate(all_y) \n",
    "        zs = jnp.concatenate(all_z) \n",
    "        \n",
    "        comp_xyz = jnp.stack([xs, ys, zs], axis=1)\n",
    "        return comp_xyz\n",
    "    \n",
    "    def final_compute_xyz(self, transformed_params):\n",
    "        all_x = []\n",
    "        all_y = []\n",
    "        all_z = []\n",
    "    \n",
    "        for param_dict in transformed_params:\n",
    "            if 'x' in param_dict:\n",
    "                all_x.append(param_dict['x'])\n",
    "            if 'y' in param_dict:\n",
    "                all_y.append(param_dict['y'])\n",
    "            if 'z' in param_dict:\n",
    "                all_z.append(param_dict['z'])\n",
    "        xs = jnp.concatenate(all_x) \n",
    "        ys = jnp.concatenate(all_y) \n",
    "        zs = jnp.concatenate(all_z) \n",
    "        return xs, ys, zs\n",
    "    def get_surface_areas(self, transformed_params):\n",
    "        '''\n",
    "        Returns (Ncomps,) array of the surface areas in um2 of all the compartments\n",
    "        JAX-compatible version\n",
    "        '''\n",
    "        all_radii = []\n",
    "        all_lengths = []\n",
    "        \n",
    "        for param_dict in transformed_params:\n",
    "            if 'radius' in param_dict:\n",
    "                all_radii.append(param_dict['radius'])\n",
    "            if 'length' in param_dict:\n",
    "                all_lengths.append(param_dict['length'])\n",
    "\n",
    "        radii = jnp.concatenate(all_radii) \n",
    "        lengths = jnp.concatenate(all_lengths) \n",
    "\n",
    "        \n",
    "        surf_areas = 2 * jnp.pi * radii * lengths\n",
    "        return surf_areas\n",
    "    \n",
    "    def create_transforms(self, name: str) -> jt.Transform:\n",
    "        \"\"\"\n",
    "        Creates robust parameter transformations using SigmoidTransform\n",
    "        for physical constraints.\n",
    "        \"\"\"\n",
    "        # --- For morphology coordinates ---\n",
    "        if name in ['x', 'y', 'z']:\n",
    "            return jt.AffineTransform(1.0, 0.0)  # No constraints\n",
    "        if name in ['radius', 'length']:\n",
    "            return jt.ChainTransform([jt.SoftplusTransform(0), jt.AffineTransform(10, 0)])\n",
    "        \n",
    "        return jt.AffineTransform(1.0, 0.0)\n",
    "        \n",
    "        \n",
    "    def setup_params(self):\n",
    "        self.cell.delete_trainables()\n",
    "        self.cell.comp(\"all\").make_trainable(\"x\")\n",
    "        self.cell.comp(\"all\").make_trainable(\"y\")\n",
    "        self.cell.comp(\"all\").make_trainable(\"z\")\n",
    "        # self.cell.comp(\"all\").make_trainable(\"radius\")\n",
    "        # self.cell.comp(\"all\").make_trainable(\"length\")\n",
    "        # self.cell.comp(\"all\").make_trainable(\"HH_eNa\")\n",
    "        # self.cell.comp(\"all\").make_trainable(\"HH_eK\")\n",
    "        # self.cell.comp(\"all\").make_trainable('axial_resistivity')\n",
    "        # self.cell.comp(\"all\").make_trainable('HH_gNa')\n",
    "        # self.cell.comp(\"all\").make_trainable('HH_gLeak')\n",
    "        # self.cell.comp(\"all\").make_trainable('gCa')\n",
    "        # self.cell.comp(\"all\").make_trainable('HH_gK')\n",
    "        # self.cell.comp(\"all\").make_trainable('gKCa')\n",
    "\n",
    "        return self.cell.get_parameters()\n",
    "\n",
    "\n",
    "    def simulate(self, params):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        delta_t = 0.05 # ms\n",
    "        t_max = 5\n",
    "        time_vec = np.arange(0, t_max+2*delta_t, delta_t)\n",
    "\n",
    "        i_delay = 1.0 \n",
    "        i_dur = 0.5\n",
    "        i_amp = 15\n",
    "        current = jx.step_current(i_delay=i_delay, i_dur=i_dur, i_amp=i_amp, delta_t=delta_t, t_max=t_max)\n",
    "\n",
    "        self.cell.delete_stimuli()\n",
    "        data_stimuli = None\n",
    "        data_stimuli = self.cell.branch(0).comp(0).data_stimulate(current)\n",
    "\n",
    "        self.cell.delete_recordings()\n",
    "        self.cell.record(\"v\")\n",
    "        self.cell.record(\"i_HH\")\n",
    "        self.cell.record(\"i_Ca\")\n",
    "        self.cell.record(\"i_KCa\")\n",
    "\n",
    "        sim_outputs = jx.integrate(self.cell, params = params, data_stimuli=data_stimuli).reshape(4, self.ncomp, -1)\n",
    "        return sim_outputs\n",
    "    \n",
    "    def gen_ei(self, outputs, transformed_params):\n",
    "        \"\"\"\n",
    "        Generate extracellular potential from simulation outputs\n",
    "        \"\"\"\n",
    "        tfd = tfp.distributions\n",
    "        seed = random.PRNGKey(0)\n",
    "\n",
    "        time_vec = np.arange(0, self.t_max+2*self.delta_t, self.delta_t)\n",
    "        current_compartment_positions = self.compute_comp_xyz(transformed_params)\n",
    "        # compartment_surface_areas = self.get_surface_areas(transformed_params) * 10E-8  # Calculate surface areas in cm^2\n",
    "        \n",
    "        current_distance = fh.distance(LITKE_519_ARRAY_GRID, current_compartment_positions) * 10E-4 \n",
    "        \n",
    "        sim_v_extra = fh.compute_eap(outputs, current_distance, self.compartment_surface_areas) \\\n",
    "             + tfd.Normal(0, 0.0001).sample((self.N_ELECTRODES, len(time_vec)), seed=seed)\n",
    "        sim_EI = self.with_ttl(sim_v_extra).T\n",
    "        return sim_EI\n",
    "    \n",
    "    def with_ttl(self, raw_data):\n",
    "        shape_array = jnp.array(raw_data.shape)\n",
    "        has_512 = jnp.any(shape_array == 512)\n",
    "        has_519 = jnp.any(shape_array == 519)\n",
    "  \n",
    "        critical_axis_512 = jnp.where(shape_array == 512, size=1, fill_value=-1)[0]\n",
    "        critical_axis_519 = jnp.where(shape_array == 519, size=1, fill_value=-1)[0]\n",
    "        \n",
    "        critical_axis = jnp.where(has_512, critical_axis_512, critical_axis_519)\n",
    "\n",
    "        append_data_shape = list(raw_data.shape)\n",
    "        \n",
    "        for i, dim_size in enumerate(raw_data.shape):\n",
    "            if dim_size == 512 or dim_size == 519:\n",
    "                critical_axis = i\n",
    "                break\n",
    "        \n",
    "        append_data_shape[critical_axis] = 1\n",
    "        append_data = jnp.zeros(append_data_shape)\n",
    "        \n",
    "        return jnp.concatenate((append_data, raw_data), axis=critical_axis)\n",
    "\n",
    "    def predict(self, params):\n",
    "        outputs = self.simulate(params)\n",
    "        predicted_ei = self.gen_ei(outputs, params)\n",
    "        return predicted_ei\n",
    "\n",
    "    def loss(self, opt_params, true_ei):\n",
    "        \"\"\"\n",
    "        Computes a loss that is robust to temporal shifts by using a\n",
    "        differentiable soft-alignment mechanism.\n",
    "        \"\"\"\n",
    "        transformed_params = self.tf.forward(opt_params)\n",
    "        predicted_ei = self.predict(transformed_params)\n",
    "\n",
    "        true_len = true_ei.shape[0]\n",
    "        pred_len = predicted_ei.shape[0]\n",
    "        n_electrodes = true_ei.shape[1]\n",
    "        max_offset = pred_len - true_len\n",
    "\n",
    "        def compute_loss_at_offset(offset):\n",
    "            pred_window = lax.dynamic_slice(\n",
    "                predicted_ei,\n",
    "                start_indices=[offset, 0],\n",
    "                slice_sizes=[true_len, n_electrodes]\n",
    "            )\n",
    "\n",
    "            epsilon = 1e-6\n",
    "            pred_norm = (pred_window - jnp.mean(pred_window)) / (jnp.std(pred_window) + epsilon)\n",
    "            true_norm = (true_ei - jnp.mean(true_ei)) / (jnp.std(true_ei) + epsilon)\n",
    "\n",
    "            mse = jnp.mean((pred_norm - true_norm)**2)\n",
    "            return mse\n",
    "\n",
    "        offsets = jnp.arange(max_offset + 1)\n",
    "        all_mse_losses = vmap(compute_loss_at_offset)(offsets)\n",
    "\n",
    "        \n",
    "        temperature = 1.0\n",
    "        weights = jax.nn.softmax(-all_mse_losses * temperature)\n",
    "\n",
    "        final_loss = jnp.sum(weights * all_mse_losses)\n",
    "\n",
    "\n",
    "        return final_loss\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        epoch_losses = []\n",
    "        for epoch in range(self.num_epochs):\n",
    "            epoch_loss = 0.0\n",
    "            \n",
    "            loss_val, gradient = self.jitted_grad(self.opt_params, self.data_point)\n",
    "            flat_grads, _ = jax.tree_util.tree_flatten(gradient)\n",
    "            if jnp.isnan(loss_val):\n",
    "                break\n",
    "            if flat_grads: \n",
    "                grad_norm = jnp.linalg.norm(jnp.concatenate([g.flatten() for g in flat_grads]))\n",
    "                print(f\"Epoch {epoch}, Loss {loss_val}, Grad Norm: {grad_norm}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch}, Loss {loss_val}, No gradients found.\")\n",
    "            updates, self.opt_state = self.optimizer.update(gradient, self.opt_state)\n",
    "            self.opt_params = optax.apply_updates(self.opt_params, updates)\n",
    "            epoch_loss += loss_val\n",
    "        \n",
    "            print(f\"epoch {epoch}, loss {epoch_loss}\")\n",
    "            epoch_losses.append(epoch_loss)\n",
    "        final_params = self.tf.forward(self.opt_params)\n",
    "        # xs,ys,zs = self.final_compute_xyz(final_params) \n",
    "        # self.cell.comp(all).move_to(xs,ys,zs, update_nodes=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        sim_ei = self.predict(final_params)\n",
    "        return final_params, sim_ei, epoch_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ten_amrith_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
